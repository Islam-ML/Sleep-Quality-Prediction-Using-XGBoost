# -*- coding: utf-8 -*-
"""Untitled60.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mJh1GKG7ThxueXlYqPqRtDaOsZz7pdVP
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from xgboost import XGBClassifier
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                        f1_score, confusion_matrix, classification_report)
import joblib

# --- 1. Load the dataset ---
# Read the CSV file containing coffee consumption and health-related data
df = pd.read_csv('/content/synthetic_coffee_health_10000.csv')

# --- 2. Exploratory Data Analysis ---
# Display sample records, general info, and descriptive statistics to understand the data
print(df.head())
print(df.info())
print(df.describe())

# --- 3. Data Cleaning and Preparation ---
# Drop unnecessary columns to improve model performance and reduce noise (e.g., ID and Country)
df.drop(['ID', 'Country'], axis=1, inplace=True)

# Convert 'Gender' column to one-hot encoded columns, dropping the first to avoid dummy variable trap
df = pd.get_dummies(df, columns=['Gender'], drop_first=True)

# Encode 'Sleep_Quality' as ordered numerical values representing sleep quality levels (Poor=0 to Excellent=3)
df['Sleep_Quality'] = df['Sleep_Quality'].map({'Poor':0, 'Fair':1, 'Good':2, 'Excellent':3})

# Encode 'Stress_Level' to numerical values for easier modeling
df['Stress_Level'] = df['Stress_Level'].map({'Low':0, 'Medium':1, 'High':2})

# Fill missing values in 'Health_Issues' with 'No' and then encode numerically
df['Health_Issues'] = df['Health_Issues'].fillna('No')
df['Health_Issues'] = df['Health_Issues'].map({'No':0, 'Mild':1, 'Moderate':2, 'Severe':3})

# One-hot encode the 'Occupation' column to transform categorical data into numeric format
df = pd.get_dummies(df, columns=['Occupation'], drop_first=True)

# --- 4. Define Features (X) and Target (y) ---
X = df.drop('Sleep_Quality', axis=1)  # All columns except the target
y = df['Sleep_Quality']               # Target variable representing sleep quality

# --- 5. Handle Class Imbalance Using SMOTE ---
# Since class imbalance may exist, use SMOTE to synthetically oversample minority classes
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# --- 6. Feature Scaling ---
# Scale features to normalize ranges and improve model training stability and performance
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_resampled)

# --- 7. Split the Dataset into Training and Testing Sets ---
# Hold out 20% of data for testing while preserving class distribution with stratify
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)

# --- 8. Initialize XGBoost Model and Define Hyperparameter Grid ---
# Create XGBoost classifier with multi-class log loss metric
model = XGBClassifier(eval_metric='mlogloss', random_state=42)

# Define grid of hyperparameters for tuning
param_grid = {
    'n_estimators': [50, 100, 150, 200],      # Number of trees
    'max_depth': [3, 5, 7, 10],               # Maximum tree depth
    'learning_rate': [0.01, 0.05, 0.1, 0.2]  # Learning rate (step size shrinkage)
}

# Setup GridSearchCV to find the best hyperparameters using 5-fold cross-validation
grid_search = GridSearchCV(
    estimator=model,
    param_grid=param_grid,
    scoring='accuracy',  # Evaluation metric
    cv=5,
    n_jobs=-1,           # Utilize all CPU cores for parallel processing
    verbose=1            # Display progress messages
)

# --- 9. Train the model using GridSearchCV ---
grid_search.fit(X_train, y_train)

# Display best hyperparameters found
print("\n=== Best Hyperparameters ===")
print(grid_search.best_params_)

# Display best cross-validation accuracy
print("\n=== Best Cross-validation Accuracy ===")
print(grid_search.best_score_)

# --- 10. Evaluate the Model on Test Data ---
y_pred = grid_search.predict(X_test)

print("\n=== Test Set Accuracy ===")
print(accuracy_score(y_test, y_pred))
print("="*50)
# Show key classification metrics: Accuracy, Precision, Recall, F1 Score
print("\n=== Evaluation Metrics ===")
print(f"Accuracy: {accuracy_score(y_test, y_pred)*100:.2f}")
print("="*50)
print(f"Precision: {precision_score(y_test, y_pred, average='weighted')*100:.2f}")
print("="*50)
print(f"Recall: {recall_score(y_test, y_pred, average='weighted')*100:.2f}")
print("="*50)
print(f"F1 Score: {f1_score(y_test, y_pred, average='weighted')*100:.2f}")

# Detailed classification report per class
print("\n=== Classification Report ===")
print(classification_report(y_test, y_pred))

# --- 11. Plot Confusion Matrix ---
# Visualize true vs predicted classes to better understand model errors
print("="*50)
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6,6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=sorted(y.unique()), yticklabels=sorted(y.unique()))
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

# --- 12. Save the trained model and scaler ---
# Save the best estimator and scaler objects for later use in inference or deployment
joblib.dump(grid_search.best_estimator_, 'xgb_best_model.joblib')
joblib.dump(scaler, 'scaler.joblib')

print("\nModel and scaler saved successfully!")

import matplotlib.pyplot as plt
from sklearn.model_selection import learning_curve
import numpy as np

def plot_learning_curve(estimator, X, y, cv=5, scoring='accuracy', train_sizes=np.linspace(0.1, 1.0, 10)):
    """
    Plot learning curve of an estimator.

    Parameters:
    - estimator: the model to evaluate (e.g., your XGBClassifier with best params)
    - X: feature matrix (scaled)
    - y: target vector
    - cv: number of folds in cross-validation
    - scoring: metric to evaluate
    - train_sizes: sizes of training subsets to use
    """

    train_sizes, train_scores, valid_scores = learning_curve(
        estimator, X, y, cv=cv, scoring=scoring, train_sizes=train_sizes, n_jobs=-1, verbose=1
    )

    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    valid_scores_mean = np.mean(valid_scores, axis=1)
    valid_scores_std = np.std(valid_scores, axis=1)

    plt.figure(figsize=(10,6))
    plt.plot(train_sizes, train_scores_mean, 'o-', color='blue', label='Training score')
    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                     train_scores_mean + train_scores_std, alpha=0.1, color='blue')

    plt.plot(train_sizes, valid_scores_mean, 'o-', color='green', label='Validation score')
    plt.fill_between(train_sizes, valid_scores_mean - valid_scores_std,
                     valid_scores_mean + valid_scores_std, alpha=0.1, color='green')

    plt.title('Learning Curve')
    plt.xlabel('Training Set Size')
    plt.ylabel(scoring.capitalize())
    plt.legend(loc='best')
    plt.grid(True)
    plt.show()

# مثال على استخدام الدالة مع أفضل نموذج وجدته في GridSearchCV
best_model = grid_search.best_estimator_  # النموذج المدرب

# استخدم البيانات اللي بعد التحجيم والتوازن
plot_learning_curve(best_model, X_scaled, y_resampled, cv=5, scoring='accuracy')